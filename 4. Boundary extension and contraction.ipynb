{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff56dd7-9567-49b8-9986-dadac44dd933",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Boundary extension and contraction\n",
    "\n",
    "This notebook contains code for exploring boundary extension and contraction in a VAE trained on the shapes3d dataset.\n",
    "\n",
    "Tested with tensorflow 2.11.0 and Python 3.10.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc03a33-8116-4a38-8b01-6782a84a7654",
   "metadata": {},
   "source": [
    "#### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b5b4a-55de-4177-93ab-9d0b85c9886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec08c9-c699-4bdc-8bcb-91ba4b9a5399",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d654d64-ae02-44f6-ad6f-35d61dff4375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from PIL import Image, ImageOps\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model, Sequential, metrics, optimizers, layers\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from utils import load_tfds_dataset\n",
    "from generative_model import encoder_network_large, decoder_network_large, VAE\n",
    "tf.keras.utils.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb574f0-27e8-4d83-ade4-f8c5167fd22d",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18735e9f-8c7d-4bbf-9943-689fc064eb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds, train_labels, test_labels = load_tfds_dataset('shapes3d', labels=True, \n",
    "                                                                 key_dict= {'shapes3d': 'label_scale'})\n",
    "train_ds = train_ds / 255\n",
    "test_ds = test_ds / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ced13",
   "metadata": {},
   "source": [
    "#### Filter to just objects of mean size\n",
    "\n",
    "We can't use the object_size attribute directly because this is not equivalent to a 'close-up' or 'far away' view - the background is still the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = [img for img, label in zip(test_ds, test_labels) if label in [4,5]]\n",
    "test_ds = np.array(test_ds)\n",
    "test_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f4ee3-3430-4a85-b730-64f3ddb19c08",
   "metadata": {},
   "source": [
    "#### Load the trained VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298c6e7-25bc-4c0b-8275-17cb4a21bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "latent_dim = 20\n",
    "input_shape = (64, 64, 3)\n",
    "\n",
    "encoder, z_mean, z_log_var = encoder_network_large(input_shape, latent_dim)\n",
    "decoder = decoder_network_large(latent_dim)\n",
    "\n",
    "encoder.load_weights(\"model_weights/shapes3d_encoder.h5\")\n",
    "decoder.load_weights(\"model_weights/shapes3d_decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3d67c-ca91-4fbd-a7bf-c5742363ca63",
   "metadata": {},
   "source": [
    "#### Test boundary extension / contraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13691a-829c-4491-aa65-f8ce1c1e1024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_border(im_as_array, border_width=5):\n",
    "    img = Image.fromarray((im_as_array*255).astype(np.uint8))\n",
    "    im_crop = ImageOps.crop(img, border=border_width)\n",
    "    new_im = im_crop.resize((64,64))\n",
    "    return np.array(new_im) / 255\n",
    "\n",
    "def add_border(img, border_width=5):\n",
    "    img = np.pad(img*255, pad_width=((border_width,border_width),\n",
    "                                     (border_width,border_width),\n",
    "                                     (0,0)), mode='edge')\n",
    "    img = Image.fromarray(img.astype(np.uint8))\n",
    "    img = img.resize((64,64))\n",
    "    return np.array(img)/255\n",
    "\n",
    "def add_noise(im_as_array):\n",
    "    img = Image.fromarray((im_as_array*255).astype(np.uint8))\n",
    "    gaussian = np.random.normal(0, 30, (img.size[0],img.size[1], 3))\n",
    "    noisy_img = img + gaussian\n",
    "    return np.clip(np.array(noisy_img), 0, 255) / 255\n",
    "\n",
    "def display_recalled(x_test_new, decoded_imgs, n=10):\n",
    "    plt.figure(figsize=(n*2, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(x_test_new[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + n + 1)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "code = Model(encoder.input, encoder.get_layer('mean').output)\n",
    "\n",
    "x_test_new = np.array([add_noise(image) for image in train_ds[0:20]])\n",
    "encoded_imgs = code.predict(x_test_new)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "display_recalled(x_test_new, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2a8a6",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19cf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "border_const = 5\n",
    "\n",
    "def plot_zoom_rows(ind):\n",
    "    x_test_new_remove = np.array([add_noise(remove_border(train_ds[ind], border_width=5.33*i)) for i in range(2)])\n",
    "    encoded_imgs = code.predict(x_test_new_remove)\n",
    "    decoded_imgs_remove_border = decoder.predict(encoded_imgs)\n",
    "\n",
    "    x_test_new_add = np.array([add_noise(add_border(train_ds[ind], border_width=8*i)) for i in range(2)])\n",
    "    encoded_imgs = code.predict(x_test_new_add)\n",
    "    decoded_imgs_add_border = decoder.predict(encoded_imgs)\n",
    "\n",
    "    display_recalled(x_test_new_add[::-1].tolist() + x_test_new_remove.tolist()[1:], \n",
    "                     decoded_imgs_add_border[::-1].tolist() + decoded_imgs_remove_border.tolist()[1:], n=3)\n",
    "\n",
    "for i in range (0,50):\n",
    "    plot_zoom_rows(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e294550-5b25-4ddf-b3fd-3150f16c9470",
   "metadata": {},
   "source": [
    "#### Measure change in object size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc2a13-1c00-4242-bced-208b6190193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image):\n",
    "    # Reshape the image to be a list of RGB pixels and convert to float32\n",
    "    pixels = image.reshape(-1, 3).astype(np.float32)\n",
    "    \n",
    "    # Define criteria and apply kmeans()\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    k = 4  # Change this to the number of color blocks you want\n",
    "    _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    \n",
    "    # Convert back to 8 bit and reshape to original image shape\n",
    "    segmented_image = centers[labels.flatten()].reshape(image.shape).astype(np.uint8)\n",
    "\n",
    "    return segmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f665e-0174-4cfc-ad69-624178e49233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_object_height(image):\n",
    "    # get the index of the middle column\n",
    "    mid_col_idx = image.shape[1] // 2\n",
    "    mid_col = image[:, mid_col_idx]\n",
    "\n",
    "    # list to hold colors and their counts\n",
    "    colors = []\n",
    "    counts = []\n",
    "\n",
    "    # iterate over the mid_col\n",
    "    for color in mid_col:\n",
    "        # convert color array to a tuple so it can be used in a list\n",
    "        color_t = tuple(color)\n",
    "        if color_t in colors:\n",
    "            # if color is already in the list, increment its count\n",
    "            idx = colors.index(color_t)\n",
    "            counts[idx] += 1\n",
    "        else:\n",
    "            # if color is new, add it to the list and start its count\n",
    "            colors.append(color_t)\n",
    "            counts.append(1)\n",
    "\n",
    "    # calculate the proportions\n",
    "    proportions = [count / len(mid_col) for count in counts]\n",
    "\n",
    "    # print and return proportions\n",
    "    print(proportions)\n",
    "    return sum(proportions[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ea31f-f5e6-4f14-9c64-b506ddf78f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zoom_levels = range(80, 121, 5)\n",
    "size_changes_dict = {zoom: [] for zoom in zoom_levels}\n",
    "\n",
    "def get_size_change_and_zoom(ind):\n",
    "    changes = []\n",
    "\n",
    "    for zoom in zoom_levels:\n",
    "        # We have: ratio = shape_width_after / shape_width_before = width / (width + 2 * margin)\n",
    "        # This means margin = (32 / ratio) - 32\n",
    "        # E.g. the margin to add for a zoom percentage of 80% (i.e. ratio of 0.8) is 8 pixels\n",
    "        border_width = abs(int((32 / (zoom / 100)) - 32))\n",
    "        if zoom < 100:\n",
    "            image = add_border(train_ds[ind], border_width=border_width)\n",
    "        elif zoom > 100:\n",
    "            image = remove_border(train_ds[ind], border_width=border_width)\n",
    "        else:\n",
    "            image = train_ds[ind]\n",
    "        \n",
    "        encoded_img = code.predict(np.array([image]))\n",
    "        decoded_img = decoder.predict(encoded_img)[0]\n",
    "\n",
    "        input_height = measure_object_height(segment_image(image*255))\n",
    "        output_height = measure_object_height(segment_image(decoded_img*255))\n",
    "\n",
    "        if input_height != 0:\n",
    "            change = (output_height - input_height) / input_height * 100  # Percentage change\n",
    "            # change = output_height / input_height * 100  # Percentage change\n",
    "        else:\n",
    "            change = 0\n",
    "        changes.append(change)\n",
    "        \n",
    "    return changes, zoom_levels\n",
    "\n",
    "size_changes = []\n",
    "zoom_changes = []\n",
    "\n",
    "for i in range(100):\n",
    "    changes, zoom_levels = get_size_change_and_zoom(i)\n",
    "    size_changes.extend(changes)\n",
    "    zoom_changes.extend(zoom_levels)\n",
    "\n",
    "# Separate size changes by zoom level\n",
    "for size_change, zoom_level in zip(size_changes, zoom_changes):\n",
    "    size_changes_dict[zoom_level].append(size_change)\n",
    "\n",
    "# Calculate means and standard deviations\n",
    "means = [np.mean(size_changes_dict[zoom]) for zoom in zoom_levels]\n",
    "std_devs = [np.std(size_changes_dict[zoom]) for zoom in zoom_levels]\n",
    "sems = [np.std(size_changes_dict[zoom]) / np.sqrt(len(size_changes_dict[zoom])) for zoom in zoom_levels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e387c3-a7bb-4408-b018-887e0dca5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot line chart with error bars\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.errorbar(zoom_levels, means, yerr=sems, fmt='-o', capsize=5)\n",
    "# plt.xticks([80, 90, 100, 110, 120], fontsize=18)\n",
    "# plt.yticks(fontsize=18)\n",
    "# plt.xlabel('Zoom Level (%)', fontsize=18)\n",
    "# plt.ylabel('Object size (output/input as %)', fontsize=18)\n",
    "# plt.savefig('BE.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a3df2-ef89-46a7-8c7f-c3e24be114be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart with error bars\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(zoom_levels, means, yerr=sems, capsize=5, width=3.5)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel('Zoom Level (%)', fontsize=18)\n",
    "plt.ylabel('Change in object size (%)', fontsize=18)\n",
    "plt.axhline(y=0, color='black', linewidth=0.8) \n",
    "ax=plt.gca()\n",
    "ax.invert_xaxis()\n",
    "plt.savefig('BE.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
