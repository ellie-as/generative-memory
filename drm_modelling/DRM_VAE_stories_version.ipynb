{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRM experiment simulation - extended model\n",
    "\n",
    "The Deese-Roediger-McDermott task is a classic way to measure memory distortion. This notebook tries to recreate the human results in VAE and AE models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.11.0\n",
    "!pip install tensorflow-datasets\n",
    "!pip install tfds-nightly\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from config import DRM_lists, lures\n",
    "from data_preparation import *\n",
    "from generative_model import *\n",
    "# from hpc import HPC\n",
    "from hopfield_models import *\n",
    "# from drm_utils import *\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, vectorizer = prepare_data(max_df=0.1, min_df=0.0005, ids=False)\n",
    "\n",
    "# vae = train_vae(x_train, vectorizer, eps=100, ld=300, beta=0.001, batch=128, l1_value=0.01,\n",
    "#                 model_save_path='300ld_100eps_0.001beta_128batch_0.01l1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = '300ld_100eps_0.001beta_128batch_0.01l1.h5'\n",
    "vae = load_vae(vectorizer, ld=300, beta=0.001, \n",
    "               model_weights_path=model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extended model - store latent codes plus IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists with additional unique spatiotemporal features:\n",
    "experiences = [' '.join([f'id_{n}'] + l) for n, l in enumerate(list(DRM_data.values()))]\n",
    "print(\"Example list with unique spatiotemporal feature:\")\n",
    "print(experiences[0])\n",
    "\n",
    "_, id_vectorizer = prepare_data(max_df=0.1, min_df=0.0005, ids=True)\n",
    "\n",
    "id_vectorizer.transform(['id_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set latent dimension\n",
    "ld = 300\n",
    "\n",
    "# create MHN with dimension ld + id_vectorizer vocabulary size\n",
    "net = ContinuousHopfield(ld + len(id_vectorizer.vocabulary_.keys()),\n",
    "                         beta=100,\n",
    "                         do_normalization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_latent_code(sent):\n",
    "    encoded = vae.encoder.predict(vectorizer.transform([sent]))[0]\n",
    "    return encoded\n",
    "\n",
    "patterns = []\n",
    "for test_text in experiences:\n",
    "    # get latent code for list\n",
    "    latent = get_latent_code(test_text)\n",
    "    # flatten latent to 1D array\n",
    "    latent = latent.flatten()  \n",
    "    # get vector representing unique spatiotemporal context\n",
    "    id_counts = id_vectorizer.transform([test_text.split()[0]]).toarray()\n",
    "    # check unique spatiotemporal context\n",
    "    print(id_vectorizer.inverse_transform(id_counts))\n",
    "    id_counts = id_counts.flatten() \n",
    "    pattern = list(latent) + list(id_counts)\n",
    "    patterns.append(pattern)\n",
    "\n",
    "patterns = [np.array(p).reshape(-1, 1) for p in patterns]\n",
    "net.learn(np.array(patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hybrid_recall(test_text, net):\n",
    "    latent = np.full((1, ld), 0)\n",
    "    latent = latent.flatten()  # flatten latent to 1D array\n",
    "    id_counts = id_vectorizer.transform([test_text]).toarray()\n",
    "    id_counts = id_counts.flatten()  # flatten id_counts to 1D array\n",
    "    pattern = list(latent) + list(id_counts)  # concatenating two lists\n",
    "\n",
    "    memory = net.retrieve(np.array(pattern).reshape(-1, 1))\n",
    "    \n",
    "    decoded = vae.decoder.predict(memory[0:ld].reshape((1,ld)))\n",
    "    top_words = [(word_lookup[index], decoded[0][index]) for index in np.argsort(-decoded)[0]][0:15]\n",
    "\n",
    "    unpredictable_component = id_vectorizer.inverse_transform(np.array(memory[ld:]).reshape((1, 4226)))\n",
    "    recalled_words = [tuple((unpredictable_component[0][0], 1))] + list(top_words)\n",
    "    return recalled_words\n",
    "\n",
    "def hybrid_plot(ax, terms, scores, clrs, lure_word):\n",
    "    ax.bar(terms, scores, color=clrs, alpha=0.5)\n",
    "    ax.axhline(y=0.5, color='grey', linestyle='--') # Add a dashed line at y=0.5\n",
    "    ax.set_ylabel('Recall score', fontsize=16)\n",
    "    ax.set_title(f\"Lure word '{lure_word}'\", fontsize=18)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=90, fontsize=16)\n",
    "\n",
    "word_lookup = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "fig, axs = plt.subplots(len(lures), 1, figsize=(10, 4*len(lures)))\n",
    "fig.tight_layout(h_pad=12)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    lure = lures[i]\n",
    "    list_words = DRM_data[lures[i]] + [f'id_{i}']\n",
    "    recalled = hybrid_recall(f'id_{i}', net)\n",
    "    terms = [i[0] for i in recalled]\n",
    "    scores = [i[1] for i in recalled]\n",
    "    clrs = ['red' if x == lures[i] else 'blue' if x in list_words else 'grey' for x in terms]\n",
    "    hybrid_plot(ax, terms, scores, clrs, lure)\n",
    "\n",
    "plt.savefig('mhn_drm.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show longer lists increase false recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_list_length(vectorizer, encoder, decoder, list_name, num_words):\n",
    "    # Get the list\n",
    "    full_list = DRM_data[list_name]\n",
    "\n",
    "    # Check if the number of words is greater than the list length\n",
    "    if num_words > len(full_list):\n",
    "        print(\"The number of words is greater than the list length.\")\n",
    "        return None\n",
    "\n",
    "    # Get the first num_words of the list\n",
    "    subset_list = full_list[:num_words]\n",
    "    print(f\"Subset list: {subset_list}\")\n",
    "\n",
    "    # Transform the list into the VAE's feature space and encode and decode it\n",
    "    encoded = encoder.predict(vectorizer.transform([' '.join(subset_list)]))[0]\n",
    "    decoded = decoder.predict(encoded)\n",
    "\n",
    "    # Get the recalled words\n",
    "    word_lookup = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    recalled_words = [word_lookup[index] for index in np.argsort(-decoded)[0] if decoded[0][index] > 0.5]\n",
    "\n",
    "    print(f\"Recalled words: {recalled_words}\")\n",
    "\n",
    "    if list_name in recalled_words:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "word_counts = range(1, 15)\n",
    "list_names = list(DRM_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# For each list\n",
    "for list_name in list_names:\n",
    "    results[list_name] = []\n",
    "    \n",
    "    # For each word count\n",
    "    for num_words in word_counts:\n",
    "        # Calculate semantic intrusions\n",
    "        semantic_intrusions = test_list_length(vectorizer, vae.encoder, vae.decoder, list_name, num_words)\n",
    "        \n",
    "        # Store the result\n",
    "        results[list_name].append(semantic_intrusions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array to hold the percentages and standard errors\n",
    "percentages = []\n",
    "errors = []\n",
    "\n",
    "# For each index and number of words\n",
    "for i, num_words in enumerate(word_counts):\n",
    "    lure_retrieved_counts = []\n",
    "    # For each list\n",
    "    for list_name in list_names:\n",
    "        # Append whether the lure word was retrieved or not\n",
    "        lure_retrieved_counts.append(results[list_name][i])\n",
    "    # Calculate the percentage and standard error, and append them to the arrays\n",
    "    percentage = np.mean(lure_retrieved_counts) * 100\n",
    "    error = np.std(lure_retrieved_counts) / np.sqrt(len(lure_retrieved_counts)) * 100\n",
    "    percentages.append(percentage)\n",
    "    errors.append(error)\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the line plot with error bars\n",
    "plt.errorbar(word_counts, percentages, yerr=errors, fmt='-o', capsize=5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Number of words\", fontsize=16)\n",
    "plt.ylabel(\"Lure recall percentage\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig('lure_words_fraction.png', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
