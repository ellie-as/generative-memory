{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f796bd0",
   "metadata": {},
   "source": [
    "### Memory distortions in the hybrid model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94923d7f",
   "metadata": {},
   "source": [
    "#### Installation:\n",
    "\n",
    "Tested with tensorflow 2.11.0 and Python 3.10.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install importlib-metadata==4.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2672f4a",
   "metadata": {},
   "source": [
    "#### Imports and set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from end_to_end import *\n",
    "from extended_model import *\n",
    "from extended_model_utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dims = (64,64,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0e459",
   "metadata": {},
   "source": [
    "#### Prepare filtered dataframe for Carmichael simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56a9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'shapes3d'\n",
    "ds = tfds.load(dataset, split='train', shuffle_files=False, data_dir='./data/')\n",
    "ds_info = tfds.builder(dataset).info\n",
    "df = tfds.as_dataframe(ds, ds_info)\n",
    "\n",
    "filtered_df = df[\n",
    "    (df['label_floor_hue'] == 1) &\n",
    "    (df['label_object_hue'] == 2) &\n",
    "    (df['label_orientation'] == 7) &\n",
    "    (df['label_scale'] == 0) &\n",
    "    (df['label_wall_hue'] == 3)\n",
    "]\n",
    "\n",
    "filtered_df['image'] = filtered_df['image']/255\n",
    "\n",
    "with open('subset.pickle', 'wb') as handle:\n",
    "    pickle.dump(filtered_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18b74c",
   "metadata": {},
   "source": [
    "#### Load previously trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290c564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set tensorflow and numpy random seeds to make outputs reproducible\n",
    "tf.random.set_seed(321)\n",
    "np.random.seed(321)\n",
    "\n",
    "# set tensorflow image format to arrays of dim (width, height, num_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "# set number of epochs (actually the max number as early stopping is enabled)\n",
    "generative_epochs = 50\n",
    "# set number of images\n",
    "num_ims = 10000\n",
    "\n",
    "# dataset specific parameters:\n",
    "# the extended model is tested with mnist only\n",
    "# a lower value of beta is set for the larger image datasets to avoid overflow errors\n",
    "# a higher latent dimension is set for the larger images to ensure good reconstruction\n",
    "# note that a larger VAE is used for shapes3d and symmetric_solids than for mnist and fashion_mnist\n",
    "params = {'extended': True,\n",
    "          'hopfield_beta': 5,\n",
    "          'latent_dim': 20}\n",
    "\n",
    "net, vae = run_end_to_end(dataset='shapes3d', \n",
    "                          generative_epochs=generative_epochs, \n",
    "                          num=num_ims, \n",
    "                          latent_dim=params['latent_dim'], \n",
    "                          interpolate=False,\n",
    "                          few_shot=False,\n",
    "                          do_vector_arithmetic=False,\n",
    "                          kl_weighting=1,\n",
    "                          lr = 0.001,\n",
    "                          hopfield_beta=params['hopfield_beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4dc62a",
   "metadata": {},
   "source": [
    "#### Testing distortions at different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5981c5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "errors = []\n",
    "counts = []\n",
    "\n",
    "for t in thresholds:\n",
    "    err, pixel_count = test_extended_model(vae=vae, dataset=dataset, threshold=t,\n",
    "                                           latent_dim=params['latent_dim'], \n",
    "                                           return_errors_and_counts=True, beta=2)\n",
    "    errors.append(err)\n",
    "    counts.append(pixel_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43479cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distortions(thresholds, errors, counts):\n",
    "    fig, ax = plt.subplots(figsize=(4,3))\n",
    "    ax2 = ax.twinx()\n",
    "\n",
    "    ax.set_ylabel(\"Reconstruction error\", color='red')\n",
    "    ax2.set_ylabel('No. of sensory feature units', color='blue')\n",
    "\n",
    "    ax.plot(thresholds, errors, color='red')\n",
    "    ax2.plot(thresholds, counts, color='blue')\n",
    "\n",
    "    plt.title('Reconstruction error and memory \\ndimension at different thresholds')\n",
    "\n",
    "    ax.set_xlabel('Threshold')\n",
    "    plt.savefig(f'hybrid_model/error_thresholds_{dataset}.png', bbox_inches = \"tight\")\n",
    "    return fig\n",
    "\n",
    "fig = plot_distortions(thresholds, errors, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7476c51",
   "metadata": {},
   "source": [
    "#### Simulating semantic distortions of episodic memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11489d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('subset.pickle', 'rb') as handle:\n",
    "    filtered_df = pickle.load(handle)\n",
    "\n",
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_to_match = ['label_floor_hue', 'label_object_hue', 'label_orientation',\n",
    "                       'label_scale', 'label_wall_hue']\n",
    "\n",
    "def get_diff_shape_im(df, i, shape_id):\n",
    "    attribute_values = df.iloc[i][attributes_to_match]\n",
    "    filtered_df = df[(df[attributes_to_match] == attribute_values).all(axis=1)]\n",
    "    new_im = filtered_df.loc[filtered_df['label_shape'] == shape_id, 'image'].iloc[0]\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b156559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_extended_model_carmichael(df, dataset='shapes3d', vae=None, beta=5, generative_epochs=100, num_ims=1000, latent_dim=10,\n",
    "                        threshold=0.01, shape_id=0):\n",
    "    dims = dims_dict[dataset]\n",
    "\n",
    "    # Split the data_indices instead of the data and labels directly\n",
    "    test_data = df['image']\n",
    "    \n",
    "    # create variant of this dataset with unusual features\n",
    "    test_data_squares = [add_white_square(d, dims) for d in test_data][0:4]\n",
    "    test_data_squares = np.stack(test_data_squares, axis=0).astype(\"float32\")[0:4]\n",
    "\n",
    "    # get reconstructed images and predicted labels for test data\n",
    "    predictions = get_predictions(test_data_squares, vae)\n",
    "    diff = get_true_pred_diff(test_data_squares, predictions)\n",
    "\n",
    "    # create MHN for predictable and unpredictable elements\n",
    "    net = ContinuousHopfield(np.prod(dims) + latent_dim, beta=beta)\n",
    "    # get elements where error is above threshold and remap to range [-1,1]\n",
    "    # first compute the average error across the three channels\n",
    "    avg_diff = np.mean(abs(diff), axis=-1, keepdims=True)\n",
    "    # next create a mask where the average error is greater than the threshold\n",
    "    mask = avg_diff > threshold\n",
    "    # finally create the output image using the mask and input images\n",
    "    sparse_to_encode = np.where(mask, (test_data_squares * 2) - 1, 0)\n",
    "\n",
    "    # get latent vectors for test memories\n",
    "    _, latents = get_recalled_ims_and_latents(vae, test_data_squares, noise_level=0)\n",
    "\n",
    "    # NEW CODE\n",
    "    ims_with_diff_shapes = [get_diff_shape_im(df, i, shape_id) for i in range(4)]\n",
    "    recons, pred_labels = get_recalled_ims_and_latents(vae, np.array(ims_with_diff_shapes), noise_level=0)\n",
    "    pred_labels = np.array([pred_labels[0][i] for i in range(len(pred_labels[0]))])\n",
    "    pred_labels = pred_labels.reshape((4, 20, 1))\n",
    "    fixed_latents = pred_labels\n",
    "       \n",
    "    # encode traces in MHN\n",
    "    a = sparse_to_encode.reshape((4, np.prod(dims), 1))\n",
    "    print(a.shape)\n",
    "    print(pred_labels.shape)\n",
    "    hpc_traces = np.concatenate((a, pred_labels), axis=1)\n",
    "    net.learn(hpc_traces[0:4])\n",
    "\n",
    "    # now visualise recall from a noisy image in the MHN\n",
    "    images_masked_np = noise(hpc_traces, noise_factor=0.3, gaussian=True)\n",
    "\n",
    "    tests = []\n",
    "    label_inputs = []\n",
    "    predictions = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for test_ind in range(4):\n",
    "        test_in = images_masked_np[test_ind].reshape(-1, 1)\n",
    "        test_out = net.retrieve(test_in, max_iter=5)\n",
    "        reconstructed = test_out[0:np.prod(dims)]\n",
    "        input_im = test_in[0:np.prod(dims)]\n",
    "\n",
    "        predictions.append(np.array(reconstructed).reshape((1, dims[0], dims[1], dims[2])))\n",
    "        tests.append(np.array(input_im).reshape((1, dims[0], dims[1], dims[2])))\n",
    "        pred_labels.append(test_out[np.prod(dims):])\n",
    "        label_inputs.append(test_in[np.prod(dims):])\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    tests = np.concatenate(tests, axis=0)\n",
    "    pred_labels = np.array(pred_labels).reshape(4, latent_dim)\n",
    "    label_inputs = np.array(label_inputs).reshape(4, latent_dim)\n",
    "\n",
    "    # for the first ten test images, display the stages of recall\n",
    "    final_outputs = []\n",
    "    for test_ind in range(4):\n",
    "        fig, final_im = recall_memories(test_data_squares, net, vae, dims, latent_dim, test_ind=test_ind,\n",
    "                                        noise_factor=0.1, threshold=threshold, return_final_im=True,\n",
    "                                        fixed_latents=fixed_latents[test_ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7ce0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_extended_model_carmichael(filtered_df, \n",
    "                               dataset='shapes3d', \n",
    "                               vae=vae, \n",
    "                               latent_dim=20,\n",
    "                               threshold = 0.2,\n",
    "                               shape_id=0)\n",
    "test_extended_model_carmichael(filtered_df, \n",
    "                               dataset='shapes3d',\n",
    "                               vae=vae, \n",
    "                               latent_dim=20,\n",
    "                               threshold = 0.2,\n",
    "                               shape_id=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
